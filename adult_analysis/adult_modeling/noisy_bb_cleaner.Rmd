---
title: "noisy_bb_cleaner"
author: "anjie"
date: "5/5/2021"
output: html_document
---

```{r}
library(tidyverse)
library(matrixStats)
library(here)

source(here("adult_modeling/scripts/grid_approximation.R"))
source(here("adult_modeling/scripts/noisy_update.R"))

```

# generate creature sequence 
this is easy fill in later 
```{r}
make_creature <- function(
  total_feature,
  complexity
){
  
}

make_dissimilar_creature <- function(
  creature, 
  dissmilar_ratio, 
){
  
}

generate_creature_sequence <- function(
  block_length, 
  deviant_number, 
  deviant_position, 
){
  
}
```

# generate creature sequence 

```{r}
noisy_observation_feature <- function(
  feature, 
  n_sample, 
  epsilon 
){
  real_features <- rep(feature, n_sample)
  noisy <- rbernoulli(p = epsilon, n = n_sample)
  return(ifelse(noisy, 1-real_features, real_features))
  
}


noisy_observation_creature <- function(
  creature, 
  n_sample, 
  epsilon
){
  sapply(creature, function(y){noisy_observation_feature(
    feature = y, 
    n_sample = n_sample, 
    epsilon = epsilon
  )})
  
}

```

## test creature 
```{r}
test_creature_background_theta <- c(0.01, 0.01, 0.01, 0.99, 0.99, 0.8)
test_creature_deviant_theta <- c(0.2, 0.8, 0.8, 0.2, 0.2, 0.9)
#test creature
# these will be a block [y_1, y_2, y_3, y_4, z_1, y_5]
y_1 <- sapply(test_creature_background_theta, function(x){rbernoulli(p = x, n = 1)})
y_2 <- sapply(test_creature_background_theta, function(x){rbernoulli(p = x, n = 1)})
y_3 <- sapply(test_creature_background_theta, function(x){rbernoulli(p = x, n = 1)})
y_4 <- sapply(test_creature_background_theta, function(x){rbernoulli(p = x, n = 1)})
z_1 <-  sapply(test_creature_deviant_theta, function(x){rbernoulli(p = x, n = 1)})
#y_5 <- sapply(test_creature_background_theta, function(x){rbernoulli(p = x, n = 1)})

# in each trial collects like 100 times
y_1_noisy_observation <- rbind(noisy_observation_creature(y_1, 20, 0.02))
y_2_noisy_observation <- rbind(noisy_observation_creature(y_2, 20, 0.02))
y_3_noisy_observation <-  rbind(noisy_observation_creature(y_3, 20, 0.02))
y_4_noisy_observation <-  rbind(noisy_observation_creature(y_4, 30, 0.2))
z_1_noisy_observation <-  rbind(noisy_observation_creature(z_1, 30, 0.2))


```

## sequential update 
```{r}
noisy_update <- function(observations,
                         grid_theta, grid_epsilon, 
                         alpha_prior = 1, beta_prior = 1, 
                         alpha_epsilon = 10, beta_epsilon = 1){
  
  observation_length
  
  
  
}
```



```{r}
grid_theta <- seq(0.01, 0.99, 0.05)
grid_epsilon <- seq(0.01, 0.99, 0.05)

three_observations <- rbind(y_1_noisy_observation, 
                            y_2_noisy_observation, 
                            y_3_noisy_observation)

updates = nrow(three_observations)


# let's just look at 10 updates first

# or big_data <- dplyr::bind_rows(datalist)
datalist = list()
for (i in seq(1, updates, 1)){
  
  post_first_update_theta_epsilon_approx <- grid_approximate_creature_with_theta_and_epsilon_initial(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = three_observations[1:i, ], 
  alpha_prior = 1, 
  beta_prior = 1,
  alpha_epsilon = 10, 
  beta_epsilon = 1
) %>% 
    mutate(update_number = i)
  
   datalist[[i]] <-  post_first_update_theta_epsilon_approx
  
  
  
}
all_updates <- dplyr::bind_rows(datalist)



```



# it looks a little reversed to me but will worry abt it later 
```{r}
y_1
y_2
y_3
all_updates %>% 
  ggplot(aes(x = theta, y = exp(log_posterior), color = update_number)) + 
  geom_point()+ 
  facet_wrap(~feature_index)
```


#KL divergence starts here 


$$EIG(y) = \sum_{y'_{t+1}} { D_{KL} ( p(\theta | y_{1..y_{t+1}}) || p(\theta | y_{1..y_{t}})  ) p(y_{t+1} | \theta) }$$
actually not so sure if it makes sense to calculate EIG based on y because we don't really know anythinga bt it 



http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf

$$D_{KL}(p(x) || q(x)) = –\sum_{x \in X} p(x) ln\frac{p(x)}{q(x)}$$
$$D_{KL}(p(x) || q(x)) = –\sum_{x \in X} p(x) * ln\frac{p(x)}{q(x)}$$

in our case it will be 

$$D_{KL}(p(\theta|z_{i+1}) || p(\theta|z_{i})) = –\sum_{x \in X} p(\theta|z_{i+1})  ln\frac{p(\theta|z_{i+1})}{p(\theta|z_{i}))}$$
maybe we can pretend theta is concrete like we've been doing? so effectively we will have 

$$D_{KL}(p(\theta|z_{i+1}) || p(\theta|z_{i})) = –\sum_{\theta \in [\theta_{1}, \theta_{2}...]} p(\theta|z_{i+1})  ln\frac{p(\theta|z_{i+1})}{p(\theta|z_{i})}$$


what the heck let's just give it a try 
```{r}
distribtuion_for_feature <- distribution_df %>% 
  filter(feature_index == 1)

second_update <- distribtuion_for_feature %>% 
  filter(update_number == 2)

first_update <- distribtuion_for_feature %>% 
  filter(update_number == 1)

grid_theta <- seq(0.01, 0.99, 0.05)
kl <- c() 
for(t in grid_theta){
  
  second_update_posterior <- second_update %>% 
    filter(theta == t) %>% 
    pull(log_posterior)
  
  first_update_posterior <- first_update %>% 
    filter(theta == t)%>% 
    pull(log_posterior)
  
  # because everything is in log
  kl_for_t <- second_update_posterior + second_update_posterior - first_update_posterior
  
  kl <- c(kl, kl_for_t)
}

-matrixStats::logSumExp(kl)
```


```{r}
get_kl_for_feature <- function(feature = 1, 
                               distribution_df = all_updates){
  
  total_update_number <- length(distribution_df %>% 
                                  distinct(update_number) %>% 
                                  pull())
  
  all_learning_step_updates <- c()
  
  for(update_i in 2:total_update_number){
    
    first_update_index <- update_i - 1
    second_update_index <- update_i 
    
    distribtuion_for_feature <- distribution_df %>% 
    filter(feature_index == feature)

    second_update <- distribtuion_for_feature %>% 
      filter(update_number == second_update_index)
    
    first_update <- distribtuion_for_feature %>% 
        filter(update_number == first_update_index)

    all_thetas <- all_updates %>% distinct(theta) %>% pull()
    kl <- c() 
    for(t in all_thetas){
        
        second_update_posterior <- second_update %>% 
          filter(theta == t) %>% 
          pull(log_posterior)
        
        first_update_posterior <- first_update %>% 
          filter(theta == t)%>% 
          pull(log_posterior)
        
        # because everything is in log
        kl_for_t <- second_update_posterior + second_update_posterior - first_update_posterior
        
        kl <- c(kl, kl_for_t)
      }
    current_step_kl <- -matrixStats::logSumExp(kl)
    all_learning_step_updates <- c(all_learning_step_updates, 
                                   current_step_kl)
  
  }
  
  learning_updates <- tibble("kl" = all_learning_step_updates) %>% 
    mutate(udpate_step = row_number() + 1)
  
  return(learning_updates)

}

kl_df <- get_kl_for_feature(feature = 1, 
                  distribution_df = all_updates) 
```

```{r}
kl_df %>% 
  ggplot(aes(x = udpate_step, y = kl), 
        ) + 
  geom_line()
```




The value within the sum is the divergence for a given event.

This is the same as the positive sum of probability of each event in P multiplied by the log of the probability of the event in P over the probability of the event in Q (e.g. the terms in the fraction are flipped). This is the more common implementation used in practice.

KL(P || Q) = sum x in X P(x) * log(P(x) / Q(x))

```{r}
all_updates
```






















```{r}
grid_theta <- seq(0.01, 0.9, 0.05)
grid_epsilon <- seq(0.01, 0.9, 0.05)


post_first_update_theta_approx <- grid_approximate_creature_with_theta_initial(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = y_1_noisy_observation, 
  alpha_prior = 1, 
  beta_prior = 1,
  alpha_epsilon = 10, 
  beta_epsilon = 1
)


post_first_update_theta_epsilon_approx <- grid_approximate_creature_with_theta_and_epsilon_initial(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = y_1_noisy_observation, 
  alpha_prior = 1, 
  beta_prior = 1,
  alpha_epsilon = 10, 
  beta_epsilon = 1
)

post_first_update_theta_approx
post_first_update_theta_epsilon_approx
```






```{r}
post_second_update_theta_approx <- grid_approximate_creature_with_theta_continuous(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = y_2_noisy_observation,  
  updated_posterior_df = post_first_update_theta_approx,
  alpha_epsilon = 10, 
  beta_epsilon = 1
)

# current having weird issue around 0.1; R refuses to recognize it is there
# leading to weird issue related to log probablity having numeric(0) and problem with normalizing 
post_second_update_theta_epsilon_approx <- grid_approximate_creature_with_theta_and_epsilon_continuous(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = y_2_noisy_observation,  
  updated_posterior_df = post_first_update_theta_approx,
  alpha_epsilon = 10, 
  beta_epsilon = 1
)
```

check if we are actually doing anything meaningful: 

```{r}
test_creature_background_theta <- c(0.2, 0.2, 0.2, 0.8, 0.8, 0.3)

first_update <- post_first_update_theta_approx %>% 
  mutate(update_number = 1)
second_update <- post_second_update_theta_approx %>% 
  mutate(update_number = 2)

two_updates <- bind_rows(first_update, second_update)

two_updates %>% 
  ggplot(aes(x = theta, 
             y = normalized_log_posterior, 
             color = update_number)) + 
  geom_point()+
  facet_wrap(~feature_index)
```
actually not entirely sure. probably because we are not approximating over epsilon as well? anyway we can try to do this one more time

```{r}


post_third_update_theta_approx <- grid_approximate_creature_with_theta_continuous(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = y_3_noisy_observation,  
  updated_posterior_df = post_second_update_theta_approx,
  alpha_epsilon = 10, 
  beta_epsilon = 1
) %>% 
  mutate(update_number = 3)


post_third_update_theta_epsilon_approx <- grid_approximate_creature_with_theta_and_epsilon_continuous(
  grid_theta = grid_theta, 
  grid_epsilon = grid_epsilon, 
  noisy_creature_observation = y_3_noisy_observation,  
  updated_posterior_df = post_first_update_theta_approx,
  alpha_epsilon = 10, 
  beta_epsilon = 1
)%>% 
  mutate(update_number = 3)




three_updates <- bind_rows(two_updates, post_third_update_theta_approx)


three_updates %>% 
  ggplot(aes(x = theta, 
             y = normalized_log_posterior, 
             color = update_number)) + 
  geom_point()+
  facet_wrap(~feature_index)

```

maybe? not entirely sure. 

look at grid over the epsilon's too
```{r}
two_updates <- bind_rows(post_first_update_theta_epsilon_approx %>% mutate(update_num = 1), 
                          post_second_update_theta_epsilon_approx %>% 
                            mutate(update_num = 2))
                          
two_updates %>% 
  ggplot(aes(x = theta, 
             y = exp(log_posterior), 
             color = update_num)) + 
  geom_point()+
  facet_wrap(~feature_index)
```

ugh might be right? not sure, but let's do the surprisal 

gal: take the surprisal for each value of theta, and take the average of those surprisals weighed by p(theta = this_particular_value_of_theta|z) (

$$p(\theta = \theta_{k} | z) $$

KL(P || Q)
Where the “||” operator indicates “divergence” or Ps divergence from Q.



https://machinelearningmastery.com/divergence-between-probability-distributions/#:~:text=KL%20divergence%20can%20be%20calculated,of%20the%20event%20in%20P.&text=The%20value%20within%20the%20sum%20is%20the%20divergence%20for%20a%20given%20event.

```{r}

```



```{r}
post_first_update_theta_approx
post_second_update_theta_approx

# we will use 
y_2_noisy_observation
# to calculate based on
y_1
post_first_update_theta_approx %>% 
  mutate(probability = exp(normalized_log_posterior)) %>% 
  ggplot(aes(x = theta, y = probability)) + 
  geom_line() +
  facet_wrap(~feature_index)
```

```{r}

calculate_surprise_from_trial <- function(observations, 
                                          posterior_df){
  
  
}

calculate_feature_suprirse_from_observation <- function(observation, new_prior_df){
  
  thetas <- new_prior_df$theta 
  lp_thetas <- new_prior_df$normalized_log_posterior 
  if (observation == 1){
    surprise <- -lp_thetas  #negative log probability
    weighted_mean_surprise <- weighted.mean(x = surprise, w = lp_thetas) 
  }else {
    # 1 - p(theta) 
    # we have log(p(theta))
    # this might cause underflow problem 
    surprise <- -log(1- exp(lp_thetas))
    
  }
  
}
```





