---
title: "R Notebook"
output: html_notebook
---


```{r}
library(tidyverse)
library(matrixStats)
```
We are learning theta, the probability of a single binary feature being active. 


Our training data are individual exemplars of the concept, e.g. sample sfrom theta, y1....yn. 

$$ p(\theta) = Beta(\alpha, \beta) $$

this defines a simple Beta-Bernoulli conjugate model, so 

$$p(y | \theta) = Bernoulli(\theta)$$

but actually we don't observe y directly. for each exemplar $$y_i$$, we can choose to gather noisy sample $$z_i1...z_it$$ from these. these samples are: 

\begin{equation}
p(z_{ij} | y_i) = 
\begin{cases}
    \epsilon, & \text{for } y_i = 0 \\
    1-\epsilon, & \text{for } y_i = 1. \\
\end{cases}
\end{equation}


# Generative direction
Get our data via the generative direction

```{r}
set.seed(1000)

# perturbs observations with probability epsilon
noisy_observation <- function(y, n = 1, epsilon = .2) {
  ys <- rep(y, n)
  noisy <- rbernoulli(p = epsilon, n = n)
  return(ifelse(noisy, 1-ys, ys))
}

n_samps <- 100

alpha_theta <- 1
beta_theta <- 1

alpha_epsilon <- 1
beta_epsilon <- 10

theta <- .7
epsilon <- .2


# our training data n observations of individual exemplars of this concept y 
y_1 = rbernoulli(p = theta, n = 1)
y_2 = rbernoulli(p = theta, n = 1)
y_3 = rbernoulli(p = theta, n = 1)

# noisy_observation(y = 1, n = 10, epsilon = .2)

z_bar = c(noisy_observation(y = y_1, n = n_samps, epsilon = epsilon),
          noisy_observation(y = y_2, n = n_samps, epsilon = epsilon),
          noisy_observation(y = y_3, n = n_samps, epsilon = epsilon))
```

# Inference direction

Functions.

```{r}
lp_theta_given_z <- function(z_bar, 
                             theta, epsilon, 
                             alpha_theta, beta_theta, 
                             alpha_epsilon, beta_epsilon) {
  lp_z_given_theta(z_bar, theta, epsilon) + 
    lp_theta(theta, alpha_theta, beta_theta) + 
    lp_epsilon(epsilon, alpha_epsilon, beta_epsilon)
}

lp_z_given_theta <- function(z_bar, theta, epsilon) {
  sum(sapply(z_bar, 
             function(x) lp_zij_given_theta(zij = x, 
                                            theta = theta, 
                                            epsilon = epsilon)))
}

lp_zij_given_theta <- function(zij, theta, epsilon) {

  matrixStats::logSumExp(
    c(lp_zij_given_y(zij, 1, epsilon) + 
        lp_yi_given_theta(1, theta),
      lp_zij_given_y(zij, 0, epsilon) + 
        lp_yi_given_theta(0, theta)))
}

lp_yi_given_theta <- function(yi, theta) {
  dbinom(yi, size = 1, prob = theta, log = TRUE)
}

lp_zij_given_y <- function(zij, yi, epsilon) {
  if (zij == yi) {
    log(1-epsilon)
  } else {
    log(epsilon)
  }
}

lp_theta <- function(theta, alpha, beta) {
  dbeta(x = theta, shape1 = alpha, shape2 = beta, 
            log = TRUE)
}

lp_epsilon <- function(theta, alpha, beta) {
  dbeta(x = theta, shape1 = alpha, shape2 = beta, 
            log = TRUE)
}
```

Testing. We generated with $\theta = .7$, let's see if that gives us a higher log prob than $\theta = .3$.

```{r}
lp_zij_given_theta(1, .3, epsilon)
lp_z_given_theta(z_bar, .3, epsilon)
lp_theta_given_z(z_bar, .3, epsilon, alpha_theta, beta_theta, alpha_epsilon, beta_epsilon)
lp_theta_given_z(z_bar, .7, epsilon, alpha_theta, beta_theta, alpha_epsilon, beta_epsilon)
```

# Grid approximation of posterior

# One parameter

First try this with just theta. 

True theta was .7.

```{r}
grid_theta <- seq(0.01, .99, .01)

unnormalized_log_posterior <- sapply(grid_theta, 
                                 function(x) 
                                   lp_theta_given_z(z_bar = z_bar, 
                                                    theta = x, 
                                                    epsilon = epsilon, 
                                                    alpha_theta = alpha_theta, 
                                                    beta_theta = beta_theta,
                                                    alpha_epsilon = alpha_epsilon, 
                                                    beta_epsilon = beta_epsilon))

plot(grid_theta, unnormalized_log_posterior)
```

Now we can normalize. 

```{r}
log_posterior = unnormalized_log_posterior - matrixStats::logSumExp(unnormalized_log_posterior)

posterior = exp(log_posterior)

qplot(grid_theta, posterior) + 
  ylab("Posterior probability") +
  xlab("Theta")
```

```{r}
posterior_entropy = sum(- posterior * log(posterior))
```

# Two parameters

```{r}
grid_theta <- seq(0.01, .99, .02)
grid_epsilon <- seq(0.01, .99, .02)

samps <- expand_grid(theta = grid_theta,
                     epsilon = grid_epsilon) 
  

samps$unnormalized_log_posterior <- mapply(function(x, y) 
                                   lp_theta_given_z(z_bar = z_bar, 
                                                    theta = x, 
                                                    epsilon = y, 
                                                    alpha_theta = alpha_theta, 
                                                    beta_theta = beta_theta,
                                                    alpha_epsilon = alpha_epsilon, 
                                                    beta_epsilon = beta_epsilon), 
                                   samps$theta, 
                                   samps$epsilon)


```

Now we can normalize and look at full posterior across theta and epsilon. 

```{r}
samps$log_posterior = samps$unnormalized_log_posterior - matrixStats::logSumExp(samps$unnormalized_log_posterior)

ggplot(samps, 
       aes(x = theta, y = log_posterior, col = epsilon, group = epsilon)) +
  geom_line() + 
  viridis::scale_color_viridis()

```

Or we could look at the conditional posterior for theta, integrating across epsilon.

```{r}
# average = sum(x) / length(x) 
# log average = logsumexp(x) * log(1/length(x))

theta_posterior <- samps %>%
  group_by(theta) %>%
  summarise(log_posterior = matrixStats::logSumExp(log_posterior) + 
              log(1/length(log_posterior))) %>%
  mutate(posterior = exp(log_posterior))

ggplot(theta_posterior, 
       aes(x = theta, y = posterior)) +
  geom_line() + 
  viridis::scale_color_viridis() +
  ylim(0,.01)
  

```

