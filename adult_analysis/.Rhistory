<<<<<<< HEAD
block_length,
dev_positions)))
) %>%
mutate(beta = map(.x = sequence,
.f = get_beta_count),
probability = map(.x = beta,
.f = get_probability),
surprise = map2(.x = probability,
.y = sequence,
.f = get_surprise),
learning_progress = map(
.x = probability,
.f = get_learning_progress
)) %>%
unnest(sequence, surprise, learning_progress) %>%
unnest(learning_progress) %>%
# figure out the corresponding trial number
group_by(subject, block_number) %>%
mutate(trial_number = row_number())
d_rt <- d %>%
select(subject, block_number, trial_number, rt, item_type, trial_type, trial_complexity) %>%
mutate(rt = rt + 500) %>%  # add the baseline back
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
#rename(real_trial_number = trial_number) %>%
select(temp_id, rt, item_type, trial_type, trial_complexity)
d_exp_sim <- d_exp_sim %>%
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
left_join(d_rt, by = "temp_id")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "surprise") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("surprise") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "learning_progress") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
theme(legend.position = "bottom") +
ylab("learning_progress") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "rt") %>%
ggplot(
aes(x=trial_number, y=log(measure_value), colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("Log looking time (ms)") +
xlab("Trial Number")  +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
group_by(trial_number, item_type, trial_complexity) %>%
summarise(
mean_rt = mean(log(rt)),
mean_kl = mean(learning_progress),
mean_surprise = mean(surprise)
) %>%
ggplot(aes(x= mean_kl, y = mean_rt)) +
geom_point() +
geom_smooth(method = "lm") +
ylab("log looking time (ms)") +
xlab("KL divergence") +
theme_classic() +
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"))
#technically we only need one sequence if we want to vary prior
df_sequence <- generate_sequence_with_parameter(
d_experiment_parameter,
200,
100,
50,
0.2,
0.6)
df_sequence
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
source(here("adult_modeling/scripts/archive/00_generate_stimuli.R"))
source(here("adult_modeling/scripts/archive/01_get_learning_measure.R"))
source(here("adult_modeling/scripts/archive/02_get_experiment_parameter.R"))
d <- read_csv(here("adult_modeling/data/processed_RTdata.csv"))
# translate the experiment sequence into model-compatible structure
d_experiment_parameter <- get_experiment_parameter(d)
d_exp_sim <- d_experiment_parameter %>%
# the block length doesn't match
mutate(
sequence = pmap(d_experiment_parameter %>% select(-c(subject, block_number)), .f = ~with(list(...),
get_block_sequence(complexity, similarity,
20,
5, 10,
0.2, 0.8,
block_length,
dev_positions)))
) %>%
mutate(beta = map(.x = sequence,
.f = get_beta_count),
probability = map(.x = beta,
.f = get_probability),
surprise = map2(.x = probability,
.y = sequence,
.f = get_surprise),
learning_progress = map(
.x = probability,
.f = get_learning_progress
)) %>%
unnest(sequence, surprise, learning_progress) %>%
unnest(learning_progress) %>%
# figure out the corresponding trial number
group_by(subject, block_number) %>%
mutate(trial_number = row_number())
d_rt <- d %>%
select(subject, block_number, trial_number, rt, item_type, trial_type, trial_complexity) %>%
mutate(rt = rt + 500) %>%  # add the baseline back
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
#rename(real_trial_number = trial_number) %>%
select(temp_id, rt, item_type, trial_type, trial_complexity)
d_exp_sim <- d_exp_sim %>%
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
left_join(d_rt, by = "temp_id")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "surprise") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("surprise") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "learning_progress") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
theme(legend.position = "bottom") +
ylab("learning_progress") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "rt") %>%
ggplot(
aes(x=trial_number, y=log(measure_value), colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("Log looking time (ms)") +
xlab("Trial Number")  +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
group_by(trial_number, item_type, trial_complexity) %>%
summarise(
mean_rt = mean(log(rt)),
mean_kl = mean(learning_progress),
mean_surprise = mean(surprise)
) %>%
ggplot(aes(x= mean_kl, y = mean_rt)) +
geom_point() +
geom_smooth(method = "lm") +
ylab("log looking time (ms)") +
xlab("KL divergence") +
theme_classic() +
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"))
d_exp_sim %>%
group_by(trial_number, item_type, trial_complexity) %>%
summarise(
mean_rt = mean(log(rt)),
mean_kl = mean(learning_progress),
mean_surprise = mean(surprise)
) %>%
ggplot(aes(x= mean_surprise, y = mean_rt)) +
geom_point() +
geom_smooth(method = "lm") +
ylab("log looking time (ms)") +
xlab("Surprise") +
theme_classic() +
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"))
d_cor <- d_exp_sim %>%
select(subject, block_number, trial_number,
trial_type, trial_complexity, item_type,
surprise, learning_progress, rt) %>%
mutate(log_rt = log(rt))
d_cor_summary <- d_cor %>%
ungroup() %>%
group_by(item_type,
trial_complexity,
trial_number) %>%
summarise(
mean_suprirse = mean(surprise),
mean_lp = mean(learning_progress),
mean_log_rt = mean(log_rt))
d_exp_sim <- d_experiment_parameter %>%
# the block length doesn't match
mutate(
sequence = pmap(d_experiment_parameter %>% select(-c(subject, block_number)), .f = ~with(list(...),
get_block_sequence(complexity, similarity,
200,
50, 100,
0.2, 0.6,
block_length,
dev_positions)))
) %>%
mutate(beta = map(.x = sequence,
.f = get_beta_count),
probability = map(.x = beta,
.f = get_probability),
surprise = map2(.x = probability,
.y = sequence,
.f = get_surprise),
learning_progress = map(
.x = probability,
.f = get_learning_progress
)) %>%
unnest(sequence, surprise, learning_progress) %>%
unnest(learning_progress) %>%
# figure out the corresponding trial number
group_by(subject, block_number) %>%
mutate(trial_number = row_number())
d_rt <- d %>%
select(subject, block_number, trial_number, rt, item_type, trial_type, trial_complexity) %>%
mutate(rt = rt + 500) %>%  # add the baseline back
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
#rename(real_trial_number = trial_number) %>%
select(temp_id, rt, item_type, trial_type, trial_complexity)
d_exp_sim <- d_exp_sim %>%
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
left_join(d_rt, by = "temp_id")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "surprise") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("surprise") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "learning_progress") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
theme(legend.position = "bottom") +
ylab("learning_progress") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "rt") %>%
ggplot(
aes(x=trial_number, y=log(measure_value), colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("Log looking time (ms)") +
xlab("Trial Number")  +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
# translate the experiment sequence into model-compatible structure
d_experiment_parameter <- get_experiment_parameter(d)
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
source(here("adult_modeling/scripts/archive/00_generate_stimuli.R"))
source(here("adult_modeling/scripts/archive/01_get_learning_measure.R"))
source(here("adult_modeling/scripts/archive/02_get_experiment_parameter.R"))
d <- read_csv(here("adult_modeling/data/processed_RTdata.csv"))
d_exp_sim <- d_experiment_parameter %>%
# the block length doesn't match
mutate(
sequence = pmap(d_experiment_parameter %>% select(-c(subject, block_number)), .f = ~with(list(...),
get_block_sequence(complexity, similarity,
200,
50, 100,
0.2, 0.6,
block_length,
dev_positions)))
) %>%
mutate(beta = map(.x = sequence,
.f = get_beta_count),
probability = map(.x = beta,
.f = get_probability),
surprise = map2(.x = probability,
.y = sequence,
.f = get_surprise),
learning_progress = map(
.x = probability,
.f = get_learning_progress
)) %>%
unnest(sequence, surprise, learning_progress) %>%
unnest(learning_progress) %>%
# figure out the corresponding trial number
group_by(subject, block_number) %>%
mutate(trial_number = row_number())
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
source(here("adult_modeling/scripts/archive/00_generate_stimuli.R"))
source(here("adult_modeling/scripts/archive/01_get_learning_measure.R"))
source(here("adult_modeling/scripts/archive/02_get_experiment_parameter.R"))
d <- read_csv(here("adult_modeling/data/processed_RTdata.csv"))
# translate the experiment sequence into model-compatible structure
d_experiment_parameter <- get_experiment_parameter(d)
d_exp_sim <- d_experiment_parameter %>%
# the block length doesn't match
mutate(
sequence = pmap(d_experiment_parameter %>% select(-c(subject, block_number)), .f = ~with(list(...),
get_block_sequence(complexity, similarity,
200,
50, 100,
0.2, 0.6,
block_length,
dev_positions)))
) %>%
mutate(beta = map(.x = sequence,
.f = get_beta_count),
probability = map(.x = beta,
.f = get_probability),
surprise = map2(.x = probability,
.y = sequence,
.f = get_surprise),
learning_progress = map(
.x = probability,
.f = get_learning_progress
)) %>%
unnest(sequence, surprise, learning_progress) %>%
unnest(learning_progress) %>%
# figure out the corresponding trial number
group_by(subject, block_number) %>%
mutate(trial_number = row_number())
d_rt <- d %>%
select(subject, block_number, trial_number, rt, item_type, trial_type, trial_complexity) %>%
mutate(rt = rt + 500) %>%  # add the baseline back
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
#rename(real_trial_number = trial_number) %>%
select(temp_id, rt, item_type, trial_type, trial_complexity)
d_exp_sim <- d_exp_sim %>%
mutate(temp_id = paste(subject, block_number, trial_number)) %>%
left_join(d_rt, by = "temp_id")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "surprise") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("surprise") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "learning_progress") %>%
ggplot(
aes(x=trial_number, y=measure_value, colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
theme(legend.position = "bottom") +
ylab("learning_progress") +
xlab("Trial Number") +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
d_exp_sim %>%
pivot_longer(cols = c(surprise, learning_progress, rt),
names_to = "measure_type",
values_to = "measure_value") %>%
filter(measure_type == "rt") %>%
ggplot(
aes(x=trial_number, y=log(measure_value), colour=item_type)) +
stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
geom_smooth(method = "lm",
formula = y ~ I(exp(1)**(-x)), se = FALSE) +
facet_grid(~trial_complexity) +
langcog::scale_color_solarized(name = "Item Type") +
ylab("Log looking time (ms)") +
xlab("Trial Number")  +
theme_classic()+
theme(
axis.text=element_text(size=12),
axis.title=element_text(size=14,face="bold"),
legend.position = "bottom")
saveRDS(d_exp_sim, file = here("connecting_data_res.RDS"))
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
source(here("adult_modeling/scripts/archive/00_generate_stimuli.R"))
source(here("adult_modeling/scripts/archive/01_get_learning_measure.R"))
source(here("adult_modeling/scripts/archive/02_get_experiment_parameter.R"))
d <- read_csv(here("adult_modeling/data/processed_RTdata.csv"))
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
source(here("adult_modeling/scripts/archive/00_generate_stimuli.R"))
source(here("adult_modeling/scripts/archive/01_get_learning_measure.R"))
source(here("adult_modeling/scripts/archive/02_get_experiment_parameter.R"))
d <- read_csv(here("adult_modeling/data/processed_RTdata.csv"))
# translate the experiment sequence into model-compatible structure
d_experiment_parameter <- get_experiment_parameter(d)
View(d_experiment_parameter)
saveRDS(d_experiment_parameter, file = here("experiment_parameter.RDS"))
=======
source(here("helper/grid_approximation.R"))
source(here("helper/get_stimuli_and_observations.R"))
num_features = 2
num_features_simple = 1
# num_features_complex = 8
trials_per_block = 8
deviant_positions = 4
dissimilarity_ratio = 0.2
epsilon = 0.1
# must satisfy: total feature > (1 + dissimilar ratio) * featureOnumber
simple_stimuli <- generate_creature_sequence(
block_length = trials_per_block,
deviant_positions = deviant_positions,  # takes a vector,
total_feature = num_features,
feature_theta = 0.8,
feature_number = num_features_simple,
dissimilar_ratio = dissimilarity_ratio)
simple_observations <-
generate_noisy_observations(block = simple_stimuli,
exposure_type = "self_paced",
short_exposure_samps = 1,
long_exposure_samps = 10,
normal_exposure_samps = 10,
epsilon = epsilon)
grid_theta <- seq(0.1, 1, 0.2)
grid_epsilon <- seq(0.1, 1, 0.2)
alpha_prior = 1
beta_prior = 1
alpha_epsilon = 10
beta_epsilon = 1
obs_matrix <- simple_observations %>%
ungroup() %>%
select(-c(trial_num, observation_num)) %>%
as.matrix()
posterior_o1 <- grid_with_theta_and_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_matrix[1,],
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
# library(reshape2)
library(tidyverse)
library(here)
library(matrixStats)
## source relevant files
source(here("helper/get_entropy.R"))
source(here("helper/get_KL_measurement.R"))
source(here("helper/get_surprise.R"))
source(here("helper/noisy_update.R"))
source(here("helper/grid_approximation.R"))
source(here("helper/get_stimuli_and_observations.R"))
num_features = 2
num_features_simple = 1
# num_features_complex = 8
trials_per_block = 8
deviant_positions = 4
dissimilarity_ratio = 0.2
epsilon = 0.1
# must satisfy: total feature > (1 + dissimilar ratio) * featureOnumber
simple_stimuli <- generate_creature_sequence(
block_length = trials_per_block,
deviant_positions = deviant_positions,  # takes a vector,
total_feature = num_features,
feature_theta = 0.8,
feature_number = num_features_simple,
dissimilar_ratio = dissimilarity_ratio)
simple_observations <-
generate_noisy_observations(block = simple_stimuli,
exposure_type = "self_paced",
short_exposure_samps = 1,
long_exposure_samps = 10,
normal_exposure_samps = 10,
epsilon = epsilon)
grid_theta <- seq(0.1, 1, 0.2)
grid_epsilon <- seq(0.1, 1, 0.2)
alpha_prior = 1
beta_prior = 1
alpha_epsilon = 10
beta_epsilon = 1
obs_matrix <- simple_observations %>%
ungroup() %>%
select(-c(trial_num, observation_num)) %>%
as.matrix()
posterior_o1 <- grid_with_theta_and_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_matrix[1,],
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
lengths <- sims %>%
group_by(idx, stimulus_idx) %>%
summarise(length = n())
set.seed(42)
# now iterate through timesteps
while (stimulus_idx <= trials_per_block) {
for (f in 1:num_features) {
# get our observation
observations[[t]] <- noisy_observation_creature(simple_stimuli[[stimulus_idx]],
epsilon = epsilon)
# book keeping
df$t[t] <- t
df$stimulus_idx[t] <- stimulus_idx
# get posterior
obs_mat <- matrix(unlist(observations), ncol = 1, byrow = TRUE)
posterior_at_t <-
grid_with_theta_and_epsilon_has_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_mat,
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
# make possible scenarios
observations[[t+1]] <- NA
observations[[t+1]][f] <- FALSE
obs_mat_plus_0 <- matrix(unlist(observations), ncol = 1, byrow = TRUE)
observations[[t+1]] <- NA
observations[[t+1]][f] <- TRUE
obs_mat_plus_1 <- matrix(unlist(observations), ncol = 1, byrow = TRUE)
# update posteriors for both scenarios
posterior_at_t_plus_1_if_0 <-
grid_with_theta_and_epsilon_has_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_mat_plus_0 %>% t(),
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
posterior_at_t_plus_1_if_1 <-
grid_with_theta_and_epsilon_has_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_mat_plus_1 %>% t(),
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
# compute EIG
df$dkl_if_0[t] <- dkl(posterior_at_t_plus_1_if_0$posterior,
posterior_at_t$posterior)
df$dkl_if_1[t] <- dkl(posterior_at_t_plus_1_if_1$posterior,
posterior_at_t$posterior)
df$post_pred[t] <- noisy_post_pred(posterior_at_t$theta,
posterior_at_t$epsilon,
posterior_at_t$posterior)
df$EIG[t] = (1-df$post_pred[t]) * df$dkl_if_0[t] + df$post_pred[t] * df$dkl_if_1[t]
# flip a coin with p_keep_looking weight
df$p_look_away[t] = C / (df$EIG[t] + C)
df$look_away[t] = rbinom(1, 1, prob = df$p_look_away[t]) == 1
# if heads, learn from another sample of the same stimulus
if (df$look_away[t]) {
stimulus_idx <- stimulus_idx + 1
}
t <- t + 1
}
}
C = .005 # expected information gain from the world
max_obs <- 100 # need to pad with NAs
# generate stimuli up front
simple_stimuli <- generate_creature_sequence(block_length = trials_per_block,
deviant_positions = deviant_positions,
total_feature = 1,
feature_theta = 0.8,
feature_number = num_features_simple,
dissimilar_ratio = dissimilarity_ratio)
# need to track the actual observations
observations <- list()
# book keeping data frame
df <- tibble(t = rep(NA,max_obs),
stimulus_idx = rep(NA,max_obs),
dkl_if_0 = rep(NA,max_obs),
dkl_if_1 = rep(NA,max_obs),
post_pred = rep(NA,max_obs),
EIG = rep(NA,max_obs),
p_look_away = rep(NA,max_obs),
look_away = rep(NA,max_obs))
# which stimulus are we looking at
stimulus_idx <- 1
t <- 1
set.seed(42)
# now iterate through timesteps
while (stimulus_idx <= trials_per_block) {
for (f in 1:num_features) {
# get our observation
observations[[t]] <- noisy_observation_creature(simple_stimuli[[stimulus_idx]],
epsilon = epsilon)
# book keeping
df$t[t] <- t
df$stimulus_idx[t] <- stimulus_idx
# get posterior
obs_mat <- matrix(unlist(observations), ncol = 1, byrow = TRUE)
posterior_at_t <-
grid_with_theta_and_epsilon_has_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_mat,
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
# make possible scenarios
observations[[t+1]] <- NA
observations[[t+1]][f] <- FALSE
obs_mat_plus_0 <- matrix(unlist(observations), ncol = 1, byrow = TRUE)
observations[[t+1]] <- NA
observations[[t+1]][f] <- TRUE
obs_mat_plus_1 <- matrix(unlist(observations), ncol = 1, byrow = TRUE)
# update posteriors for both scenarios
posterior_at_t_plus_1_if_0 <-
grid_with_theta_and_epsilon_has_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_mat_plus_0 %>% t(),
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
posterior_at_t_plus_1_if_1 <-
grid_with_theta_and_epsilon_has_epsilon(grid_theta = grid_theta,
grid_epsilon = grid_epsilon,
noisy_observation = obs_mat_plus_1 %>% t(),
alpha_prior = alpha_prior,
beta_prior= beta_prior,
alpha_epsilon = alpha_epsilon,
beta_epsilon = beta_epsilon)
# compute EIG
df$dkl_if_0[t] <- dkl(posterior_at_t_plus_1_if_0$posterior,
posterior_at_t$posterior)
df$dkl_if_1[t] <- dkl(posterior_at_t_plus_1_if_1$posterior,
posterior_at_t$posterior)
df$post_pred[t] <- noisy_post_pred(posterior_at_t$theta,
posterior_at_t$epsilon,
posterior_at_t$posterior)
df$EIG[t] = (1-df$post_pred[t]) * df$dkl_if_0[t] + df$post_pred[t] * df$dkl_if_1[t]
# flip a coin with p_keep_looking weight
df$p_look_away[t] = C / (df$EIG[t] + C)
df$look_away[t] = rbinom(1, 1, prob = df$p_look_away[t]) == 1
# if heads, learn from another sample of the same stimulus
if (df$look_away[t]) {
stimulus_idx <- stimulus_idx + 1
}
t <- t + 1
}
}
df<-data.frame("hi","bye")
names(df)<-c("hello","goodbye")
de<-data.frame("hola","ciao")
names(de)<-c("hello","goodbye")
newdf <- rbind(df, de)
newdf
?kl
x = c(0.5, 0.5)
y = c(0.7, 0.3)
sum(x * log(x / y))
sum(x * log(x / y)) + sum(x * log(x / y))
y = c(0.7, 0.7, 0.3 0.3)
y = c(0.7, 0.7, 0.3, 0.3)
x = c(0.5, 0.5 0.5, 0.5)
x = c(0.5, 0.5, 0.5, 0.5)
sum(x * log(x / y))
theta_f1 <- c(0.3, 0.7)
p_theta_f1 <- c(0.5, 0.5)
theta_f2 <- c(0.3, 0.7)
p_theta_f2 <- c(0.6, 0.4)
p_theta_f1_tplus1 <- c(0.6, 0.4)
p_theta_f2_tplus1 <- c(0.6, 0.4)
dkl <- function (x,y) {
sum(x * log(x / y))
}
dkl(p_theta_f1, p_theta_f1_tplus1) + dkl(p_theta_f2, p_theta_f2_tplus1)
p_theta = (0.3*0.3, 0.21, 0.49, 0.21)
p_theta = (0.09, 0.21, 0.49, 0.21)
p_theta = c(0.09, 0.21, 0.49, 0.21)
sum(p_tehta)
sum(p_theta)
p_theta_plus1 = c(0.6 * 0.6, 0.4*0.4, 0.6*0.4, 0.4*0.6)
dkl(p_theta, p_theta_plus1)
p_theta = (0.5 * 0.6, 0.5*0.4, 0.5*0.6, 0.5*0.4)
pp_theta = c(0.5 * 0.6, 0.5*0.4, 0.5*0.6, 0.5*0.4)
p_theta = c(0.5 * 0.6, 0.5*0.4, 0.5*0.6, 0.5*0.4)
sum(p_theta)
dkl(p_theta_f1, p_theta_f1_tplus1) + dkl(p_theta_f2, p_theta_f2_tplus1)
dkl(p_theta, p_theta_plus1)
p_theta = c(0.5*0.6, 0.5*0.4, 0.5*0.6, 0.5*0.4)
p_theta_plus1 = c(0.6*0.6, 0.6*0.4, 0.4*0.6, 0.4*0.4)
dkl(p_theta, p_theta_plus1)
# Adrian Maries
# Convert .csv files exported by the Ruby script "Export.rb" into looking times.
# ------------------------------------------------------------------------------------------------------------------
# Set the working directory (where the R script is). The script assumes the input and output directories are inside.
# The input directory has to be named "InputFiles" and contain the .csv files output by the "Export.rb" script.
# The output directory has to be named "OutputFiles" and is where the script will output the looking times files.
workingDir <- "/Users/halieolson/Desktop/Datavyu"
setwd(workingDir)
# Get the file names of all files in the input directory.
inputFileList <- list.files(file.path(workingDir, "InputFiles"))
# Go through the files in the input directory to get the looking times from them.
for (fileName in inputFileList) {
# Read the data from the current input file.
changeDetData <- read.csv(file.path(workingDir, "InputFiles", fileName))
# Get the trial list from the "Trials_ordinal" column and create vectors for storing the looking times.
trialList <- unique(changeDetData$Trials_ordinal)
trialType <- vector(mode = class(trialList), length = length(trialList))
yLooks <- vector(mode = class(trialList), length = length(trialList))
nLooks <- vector(mode = class(trialList), length = length(trialList))
eLooks <- vector(mode = class(trialList), length = length(trialList))
# Go through the trials and compute the corresponding looking times and add trial types.
for (j in trialList) {
trialType[j] <-changeDetData[changeDetData$Trials_ordinal == trialList[j],]
# Get the "y" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
yLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "y" & changeDetData$Trials_ordinal == trialList[j],]
yLooks[j] <- sum(yLooksDataFrame$Looks_offset) - sum(yLooksDataFrame$Looks_onset)
# Get the "n" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
nLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "n" & changeDetData$Trials_ordinal == trialList[j],]
nLooks[j] <- sum(nLooksDataFrame$Looks_offset) - sum(nLooksDataFrame$Looks_onset)
# Get the "e" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
eLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "e" & changeDetData$Trials_ordinal == trialList[j],]
eLooks[j] <- sum(eLooksDataFrame$Looks_offset) - sum(eLooksDataFrame$Looks_onset)
}
# Create a data frame with the looking times and write it to the output folder.
outputDataFrame <- data.frame(trialList, trialtype, yLooks / 1000, nLooks / 1000, eLooks / 1000)
names(outputDataFrame) <- c("Trial Number", "TrialType", "Looks On (s)", "Looks Off (s)", "Looks Error (s)")
write.csv(outputDataFrame, file.path(workingDir, "OutputFiles", fileName))
}
# Adrian Maries
# Convert .csv files exported by the Ruby script "Export.rb" into looking times.
# ------------------------------------------------------------------------------------------------------------------
# Set the working directory (where the R script is). The script assumes the input and output directories are inside.
# The input directory has to be named "InputFiles" and contain the .csv files output by the "Export.rb" script.
# The output directory has to be named "OutputFiles" and is where the script will output the looking times files.
workingDir <- "/Users/galraz1/Developer/Datavyu"
setwd(workingDir)
# Get the file names of all files in the input directory.
inputFileList <- list.files(file.path(workingDir, "InputFiles"))
# Go through the files in the input directory to get the looking times from them.
for (fileName in inputFileList) {
# Read the data from the current input file.
changeDetData <- read.csv(file.path(workingDir, "InputFiles", fileName))
# Get the trial list from the "Trials_ordinal" column and create vectors for storing the looking times.
trialList <- unique(changeDetData$Trials_ordinal)
trialType <- vector(mode = class(trialList), length = length(trialList))
yLooks <- vector(mode = class(trialList), length = length(trialList))
nLooks <- vector(mode = class(trialList), length = length(trialList))
eLooks <- vector(mode = class(trialList), length = length(trialList))
# Go through the trials and compute the corresponding looking times and add trial types.
for (j in trialList) {
trialType[j] <-changeDetData[changeDetData$Trials_ordinal == trialList[j],]
# Get the "y" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
yLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "y" & changeDetData$Trials_ordinal == trialList[j],]
yLooks[j] <- sum(yLooksDataFrame$Looks_offset) - sum(yLooksDataFrame$Looks_onset)
# Get the "n" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
nLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "n" & changeDetData$Trials_ordinal == trialList[j],]
nLooks[j] <- sum(nLooksDataFrame$Looks_offset) - sum(nLooksDataFrame$Looks_onset)
# Get the "e" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
eLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "e" & changeDetData$Trials_ordinal == trialList[j],]
eLooks[j] <- sum(eLooksDataFrame$Looks_offset) - sum(eLooksDataFrame$Looks_onset)
}
# Create a data frame with the looking times and write it to the output folder.
outputDataFrame <- data.frame(trialList, trialtype, yLooks / 1000, nLooks / 1000, eLooks / 1000)
names(outputDataFrame) <- c("Trial Number", "TrialType", "Looks On (s)", "Looks Off (s)", "Looks Error (s)")
write.csv(outputDataFrame, file.path(workingDir, "OutputFiles", fileName))
}
rm(list = ls())
# Adrian Maries
# Convert .csv files exported by the Ruby script "Export.rb" into looking times.
# ------------------------------------------------------------------------------------------------------------------
# Set the working directory (where the R script is). The script assumes the input and output directories are inside.
# The input directory has to be named "InputFiles" and contain the .csv files output by the "Export.rb" script.
# The output directory has to be named "OutputFiles" and is where the script will output the looking times files.
workingDir <- "/Users/galraz1/Developer/Datavyu"
setwd(workingDir)
# Get the file names of all files in the input directory.
inputFileList <- list.files(file.path(workingDir, "InputFiles"))
# Go through the files in the input directory to get the looking times from them.
for (fileName in inputFileList) {
# Read the data from the current input file.
changeDetData <- read.csv(file.path(workingDir, "InputFiles", fileName))
# Get the trial list from the "Trials_ordinal" column and create vectors for storing the looking times.
trialList <- unique(changeDetData$Trials_ordinal)
trialType <- vector(mode = class(trialList), length = length(trialList))
yLooks <- vector(mode = class(trialList), length = length(trialList))
nLooks <- vector(mode = class(trialList), length = length(trialList))
eLooks <- vector(mode = class(trialList), length = length(trialList))
# Go through the trials and compute the corresponding looking times and add trial types.
for (j in trialList) {
trialType[j] <-changeDetData[changeDetData$Trials_ordinal == trialList[j],]
# Get the "y" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
yLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "y" & changeDetData$Trials_ordinal == trialList[j],]
yLooks[j] <- sum(yLooksDataFrame$Looks_offset) - sum(yLooksDataFrame$Looks_onset)
# Get the "n" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
nLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "n" & changeDetData$Trials_ordinal == trialList[j],]
nLooks[j] <- sum(nLooksDataFrame$Looks_offset) - sum(nLooksDataFrame$Looks_onset)
# Get the "e" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
eLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "e" & changeDetData$Trials_ordinal == trialList[j],]
eLooks[j] <- sum(eLooksDataFrame$Looks_offset) - sum(eLooksDataFrame$Looks_onset)
}
# Create a data frame with the looking times and write it to the output folder.
outputDataFrame <- data.frame(trialList, trialtype, yLooks / 1000, nLooks / 1000, eLooks / 1000)
names(outputDataFrame) <- c("Trial Number", "TrialType", "Looks On (s)", "Looks Off (s)", "Looks Error (s)")
write.csv(outputDataFrame, file.path(workingDir, "OutputFiles", fileName))
}
View(changeDetData)
View(changeDetData)
View(trialType)
View(changeDetData)
View(changeDetData)
j = 1
changeDetData[changeDetData$Trials_ordinal == trialList[j]]
changeDetData[changeDetData$Trials_ordinal == trialList[j]]$Trials_x
j
changeDetData[changeDetData$Trials_ordinal == trialList[j]]$Trials_x
changeDetData[changeDetData$Trials_ordinal == trialList[j]]$Trials_x[j]
changeDetData[changeDetData$Trials_ordinal == trialList[2]]$Trials_x
trialList
trialList[j]
trialList[2]
changeDetData$Trials_ordinal
changeDetData$Trials_ordinal == trialList[2]
changeDetData[changeDetData$Trials_ordinal == trialList[2]]
changeDetData[changeDetData$Trials_ordinal == 1]
changeDetData$Trials_ordinal == 1
changeDetData[changeDetData$Trials_ordinal == 1]
changeDetData$Trials_ordinal == 1
changeDetData$Trials_x
changeDetData$Trials_x[changeDetData$Trials_ordinal == 1]
changeDetData$Trials_x[changeDetData$Trials_ordinal == 2]
changeDetData$Trials_x[changeDetData$Trials_ordinal == 3]
changeDetData$Trials_x[changeDetData$Trials_ordinal == 4]
changeDetData$Trials_x[changeDetData$Trials_ordinal == 4][1]
# Adrian Maries
# Convert .csv files exported by the Ruby script "Export.rb" into looking times.
# ------------------------------------------------------------------------------------------------------------------
# Set the working directory (where the R script is). The script assumes the input and output directories are inside.
# The input directory has to be named "InputFiles" and contain the .csv files output by the "Export.rb" script.
# The output directory has to be named "OutputFiles" and is where the script will output the looking times files.
workingDir <- "/Users/galraz1/Developer/Datavyu"
setwd(workingDir)
# Get the file names of all files in the input directory.
inputFileList <- list.files(file.path(workingDir, "InputFiles"))
# Go through the files in the input directory to get the looking times from them.
for (fileName in inputFileList) {
# Read the data from the current input file.
changeDetData <- read.csv(file.path(workingDir, "InputFiles", fileName))
# Get the trial list from the "Trials_ordinal" column and create vectors for storing the looking times.
trialList <- unique(changeDetData$Trials_ordinal)
trialType <- vector(mode = class(trialList), length = length(trialList))
yLooks <- vector(mode = class(trialList), length = length(trialList))
nLooks <- vector(mode = class(trialList), length = length(trialList))
eLooks <- vector(mode = class(trialList), length = length(trialList))
# Go through the trials and compute the corresponding looking times and add trial types.
for (j in trialList) {
trialType[j] = changeDetData$Trials_x[changeDetData$Trials_ordinal == trialList[j]][1]
# Get the "y" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
yLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "y" & changeDetData$Trials_ordinal == trialList[j],]
yLooks[j] <- sum(yLooksDataFrame$Looks_offset) - sum(yLooksDataFrame$Looks_onset)
# Get the "n" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
nLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "n" & changeDetData$Trials_ordinal == trialList[j],]
nLooks[j] <- sum(nLooksDataFrame$Looks_offset) - sum(nLooksDataFrame$Looks_onset)
# Get the "e" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
eLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "e" & changeDetData$Trials_ordinal == trialList[j],]
eLooks[j] <- sum(eLooksDataFrame$Looks_offset) - sum(eLooksDataFrame$Looks_onset)
}
# Create a data frame with the looking times and write it to the output folder.
outputDataFrame <- data.frame(trialList, trialType, yLooks / 1000, nLooks / 1000, eLooks / 1000)
names(outputDataFrame) <- c("Trial Number", "Trial Type", "Looks On (s)", "Looks Off (s)", "Looks Error (s)")
write.csv(outputDataFrame, file.path(workingDir, "OutputFiles", fileName))
}
trialType
changeDetData$Trials_x[changeDetData$Trials_ordinal == trialList[j]][1]
changeDetData$Trials_x[changeDetData$Trials_ordinal == trialList[j]][1]
x <- changeDetData$Trials_x[changeDetData$Trials_ordinal == trialList[j]][1]
x
?as.character
trialType[j] = as.character(changeDetData$Trials_x[changeDetData$Trials_ordinal == trialList[j]][1])
trialType
# Adrian Maries
# Convert .csv files exported by the Ruby script "Export.rb" into looking times.
# ------------------------------------------------------------------------------------------------------------------
# Set the working directory (where the R script is). The script assumes the input and output directories are inside.
# The input directory has to be named "InputFiles" and contain the .csv files output by the "Export.rb" script.
# The output directory has to be named "OutputFiles" and is where the script will output the looking times files.
workingDir <- "/Users/galraz1/Developer/Datavyu"
setwd(workingDir)
# Get the file names of all files in the input directory.
inputFileList <- list.files(file.path(workingDir, "InputFiles"))
# Go through the files in the input directory to get the looking times from them.
for (fileName in inputFileList) {
# Read the data from the current input file.
changeDetData <- read.csv(file.path(workingDir, "InputFiles", fileName))
# Get the trial list from the "Trials_ordinal" column and create vectors for storing the looking times.
trialList <- unique(changeDetData$Trials_ordinal)
trialType <- vector(mode = class(trialList), length = length(trialList))
yLooks <- vector(mode = class(trialList), length = length(trialList))
nLooks <- vector(mode = class(trialList), length = length(trialList))
eLooks <- vector(mode = class(trialList), length = length(trialList))
# Go through the trials and compute the corresponding looking times and add trial types.
for (j in trialList) {
trialType[j] = as.character(changeDetData$Trials_x[changeDetData$Trials_ordinal == trialList[j]][1])
# Get the "y" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
yLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "y" & changeDetData$Trials_ordinal == trialList[j],]
yLooks[j] <- sum(yLooksDataFrame$Looks_offset) - sum(yLooksDataFrame$Looks_onset)
# Get the "n" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
nLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "n" & changeDetData$Trials_ordinal == trialList[j],]
nLooks[j] <- sum(nLooksDataFrame$Looks_offset) - sum(nLooksDataFrame$Looks_onset)
# Get the "e" looks corresponding to the current trial and get the total duration (by subtracting onsets from offsets).
eLooksDataFrame <- changeDetData[changeDetData$Looks_direction == "e" & changeDetData$Trials_ordinal == trialList[j],]
eLooks[j] <- sum(eLooksDataFrame$Looks_offset) - sum(eLooksDataFrame$Looks_onset)
}
# Create a data frame with the looking times and write it to the output folder.
outputDataFrame <- data.frame(trialList, trialType, yLooks / 1000, nLooks / 1000, eLooks / 1000)
names(outputDataFrame) <- c("Trial Number", "Trial Type", "Looks On (s)", "Looks Off (s)", "Looks Error (s)")
write.csv(outputDataFrame, file.path(workingDir, "OutputFiles", fileName))
}
>>>>>>> e38b9bf240df0ef791c1f058c33f87e900f684e9
